trainer:
  _target_: pytorch_lightning.Trainer
  checkpoint_callback: true
  callbacks: null
  default_root_dir: null
  gradient_clip_val: 0.0
  process_position: 0
  num_nodes: 1
  num_processes: 1
  gpus: 4
  auto_select_gpus: false
  tpu_cores: null
  log_gpu_memory: null
  progress_bar_refresh_rate: 1
  overfit_batches: 0.0
  track_grad_norm: -1
  check_val_every_n_epoch: 10
  fast_dev_run: ${debug}
  accumulate_grad_batches: 1
  max_epochs: 1000
  min_epochs: 1
  max_steps: null
  min_steps: null
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0
  val_check_interval: 1.0
  flush_logs_every_n_steps: 100
  log_every_n_steps: 50
  sync_batchnorm: true
  precision: 16
  weights_summary: top
  weights_save_path: null
  num_sanity_val_steps: 2
  truncated_bptt_steps: null
  resume_from_checkpoint: null
  profiler: null
  benchmark: false
  deterministic: true
  reload_dataloaders_every_epoch: false
  auto_lr_find: false
  replace_sampler_ddp: true
  terminate_on_nan: false
  auto_scale_batch_size: false
  prepare_data_per_node: true
  amp_backend: native
  amp_level: O2
  move_metrics_to_cpu: false
  accelerator: ddp
dataset:
  _target_: lasaft.data.data_provider.DataProvider
  musdb_root: ${data_dir}
  batch_size: 4
  num_workers: 0
  pin_memory: true
  num_frame: 128
  hop_length: 1024
  n_fft: 2048
  multi_source_training: false
callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: val_loss
    save_top_k: 5
    save_last: true
    mode: min
    verbose: false
    dirpath: checkpoints/
    filename: '{epoch:02d}'
    save_on_train_epoch_end: false
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val_loss
    patience: 300
    mode: min
    min_delta: 0.05
    check_on_train_epoch_end: false
model:
  lr: 0.0002
  optimizer: adam
  initializer: kaiming
  name: v2_large
  _target_: lasaft.models.conditioned.base_framework.AbstractLaSAFTNet
  query_listen: true
  key_listen: true
  n_fft: 4096
  num_frame: 128
  hop_length: 1024
  spec_type: complex
  spec_est_mode: mapping
  train_loss: spec_mse
  val_loss: raw_l1
  norm: bn
  spec2spec:
    _target_: lasaft.models.conditioned.spec2spec.base.BaseNet
    n_fft: ${model.n_fft}
    input_channels: 4
    internal_channels: 24
    n_blocks: 9
    n_internal_layers: 5
    first_conv_activation: relu
    last_activation: identity
    control_vector_type: embedding
    control_input_dim: 4
    embedding_dim: 64
    condition_to: decoder
    norm: ${model.norm}
    mk_block_f:
      _target_: lasaft.models.conditioned.spec2spec.block_functions.MkTFCLightSAFTGPoCM
      gr: ${model.spec2spec.internal_channels}
      num_layers: ${model.spec2spec.n_internal_layers}
      kt: 3
      kf: 3
      bn_factor: 16
      min_bn_units: 16
      bias: false
      activation: relu
      condition_dim: ${model.spec2spec.embedding_dim}
      num_tdfs: 16
      dk: 32
      norm: ${model.norm}
      query_listen: ${model.query_listen}
      key_listen: ${model.key_listen}
    mk_ds_f:
      _target_: lasaft.models.conditioned.spec2spec.block_functions.MKConvDS
      activation: relu
      norm: ${model.norm}
    mk_us_f:
      _target_: lasaft.models.conditioned.spec2spec.block_functions.MkConvTransposeUS
      activation: relu
      norm: ${model.norm}
logger:
  wandb:
    _target_: pytorch_lightning.loggers.wandb.WandbLogger
    project: lasaft
    name: null
    save_dir: .
    offline: false
    id: null
    log_model: false
    prefix: ''
    job_type: train
    group: ''
    tags: []
    reinit: false
debug: false
seed: 2021
print_config: true
ignore_warnings: true
work_dir: ${hydra:runtime.cwd}
data_dir: ${oc.env:data_dir}
dev_data_dir: ${oc.env:dev_data_dir}
wandb_api_key: ${oc.env:wandb_api_key}